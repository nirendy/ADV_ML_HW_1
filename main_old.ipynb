{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "\n",
    "# Task 1: Review LRA Dataset Subsets\n",
    "# Action: Look at the descriptions of the six subsets in the LRA dataset.\n",
    "# Time Estimate: 30 minutes\n",
    "# Outcome: Choose one subset for the assignment.\n",
    "# Chosen Dataset: ListOps\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# Task 2: Set Up Google Colab Environment\n",
    "# Action: Open Google Colab and set up a new notebook.\n",
    "# Time Estimate: 30 minutes\n",
    "# Outcome: Colab notebook ready for coding.\n",
    "# !pip install torch torchvision transformers\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# Task 3: Implement LSTM Class\n",
    "# Action: Write the class for an LSTM model from scratch.\n",
    "# Define input size, hidden layers, and output size.\n",
    "# Time Estimate: 2 hours\n",
    "# Outcome: Basic LSTM class implemented.\n",
    "\n",
    "\n",
    "# Simple LSTM Cell Implementation\n",
    "\n",
    "# %%\n",
    "# Task 5: Test LSTM with Dummy Data\n",
    "# Action: Run the LSTM model with some dummy data to ensure it's working.\n",
    "# Time Estimate: 1 hour\n",
    "# Outcome: Verified that the LSTM model runs without errors.\n",
    "\n",
    "\n",
    "# %%\n",
    "# Task 6: Implement Transformer Class\n",
    "# Action: Write the class for a Transformer model from scratch.\n",
    "# Define model parameters like the number of heads, layers, and embedding size.\n",
    "# Time Estimate: 3 hours\n",
    "# Outcome: Basic Transformer class implemented.\n",
    "\n",
    "# Transformer Encoder Layer Implementation\n",
    "# Transformer Encoder Layer Implementation\n",
    "\n",
    "\n",
    "# %%\n",
    "# Task 8: Test Transformer with Dummy Data\n",
    "# Action: Run the Transformer model with dummy data to ensure it's working.\n",
    "# Time Estimate: 1 hour\n",
    "# Outcome: Verified that the Transformer model runs without errors.\n",
    "\n",
    "\n",
    "# %%\n",
    "# Task 9: Implement S4 Class\n",
    "# Action: Follow the setup guidelines from the S4 GitHub repository and write the class for S4 from scratch.\n",
    "# Time Estimate: 3 hours\n",
    "# Outcome: Basic S4 class implemented.\n",
    "\n",
    "\n",
    "# Simplified S4 Architecture Implementation\n",
    "\n",
    "\n",
    "# %%\n",
    "# Task 11: Test S4 with Dummy Data\n",
    "# Action: Run the S4 model with dummy data to ensure it's working.\n",
    "# Time Estimate: 1 hour\n",
    "# Outcome: Verified that the S4 model runs without errors.\n",
    "\n",
    "\n",
    "# %%\n",
    "# Task 12: Prepare LRA Dataset Subset\n",
    "# Action: Load the chosen LRA subset into the Colab notebook.\n",
    "# Time Estimate: 1 hour\n",
    "# Outcome: LRA subset data ready for training.\n",
    "\n",
    "\n",
    "# %%\n",
    "# Task 13: Write Training Loop for LSTM\n",
    "# Action: Implement a training loop to train the LSTM model.\n",
    "# Time Estimate: 2 hours\n",
    "# Outcome: Training loop for LSTM ready.\n",
    "\n",
    "# %%\n",
    "# Task 14: Train LSTM Model Directly on Task\n",
    "# Action: Train the LSTM model on the chosen LRA subset.\n",
    "# Time Estimate: 4 hours (monitor progress)\n",
    "# Outcome: Trained LSTM model and recorded metrics.\n",
    "\n",
    "# %%\n",
    "# Task 15: Download WikiText Dataset\n",
    "# Action: Load the WikiText dataset into the Colab notebook.\n",
    "# Time Estimate: 1 hour\n",
    "# Outcome: WikiText dataset ready for pretraining.\n",
    "\n",
    "# %%\n",
    "# Task 16: Pretrain LSTM on WikiText\n",
    "# Action: Pretrain the LSTM model on the WikiText dataset.\n",
    "# Time Estimate: 4 hours (monitor progress)\n",
    "# Outcome: Pretrained LSTM model.\n",
    "\n",
    "# %%\n",
    "# Task 17: Fine-tune LSTM on LRA Subset\n",
    "# Action: Fine-tune the pretrained LSTM model on the chosen LRA subset.\n",
    "# Time Estimate: 2 hours\n",
    "# Outcome: Fine-tuned LSTM model and recorded metrics.\n",
    "\n",
    "# %%\n",
    "# Task 18: Write Training Loop for Transformer\n",
    "# Action: Implement a training loop to train the Transformer model.\n",
    "# Time Estimate: 2 hours\n",
    "# Outcome: Training loop for Transformer ready.\n",
    "\n",
    "# %%\n",
    "# Task 19: Train Transformer Model Directly on Task\n",
    "# Action: Train the Transformer model on the chosen LRA subset.\n",
    "# Time Estimate: 4 hours (monitor progress)\n",
    "# Outcome: Trained Transformer model and recorded metrics.\n",
    "\n",
    "# %%\n",
    "# Task 20: Pretrain Transformer on WikiText\n",
    "# Action: Pretrain the Transformer model on the WikiText dataset.\n",
    "# Time Estimate: 4 hours (monitor progress)\n",
    "# Outcome: Pretrained Transformer model.\n",
    "\n",
    "# %%\n",
    "# Task 21: Fine-tune Transformer on LRA Subset\n",
    "# Action: Fine-tune the pretrained Transformer model on the chosen LRA subset.\n",
    "# Time Estimate: 2 hours\n",
    "# Outcome: Fine-tuned Transformer model and recorded metrics.\n",
    "\n",
    "# %%\n",
    "# Task 22: Write Training Loop for S4\n",
    "# Action: Implement a training loop to train the S4 model.\n",
    "# Time Estimate: 2 hours\n",
    "# Outcome: Training loop for S4 ready.\n",
    "\n",
    "# %%\n",
    "# Task 23: Train S4 Model Directly on Task\n",
    "# Action: Train the S4 model on the chosen LRA subset.\n",
    "# Time Estimate: 4 hours (monitor progress)\n",
    "# Outcome: Trained S4 model and recorded metrics.\n",
    "\n",
    "# %%\n",
    "# Task 24: Pretrain S4 on WikiText\n",
    "# Action: Pretrain the S4 model on the WikiText dataset.\n",
    "# Time Estimate: 4 hours (monitor progress)\n",
    "# Outcome: Pretrained S4 model.\n",
    "\n",
    "\n",
    "# %%\n",
    "# Task 25: Fine-tune S4 on LRA Subset\n",
    "# Action: Fine-tune the pretrained S4 model on the chosen LRA subset.\n",
    "# Time Estimate: 2 hours\n",
    "# Outcome: Fine-tuned S4 model and recorded metrics.\n",
    "\n",
    "\n",
    "# %%\n",
    "# Task 26: Evaluate All Models\n",
    "# Action: Run evaluations on all trained models and collect metrics.\n",
    "# Time Estimate: 3 hours\n",
    "# Outcome: Comprehensive metrics for all models and training strategies.\n",
    "\n",
    "\n",
    "# %%\n",
    "# Task 27: Create Comparison Table\n",
    "# Action: Summarize the metrics in a comparison table.\n",
    "# Time Estimate: 1 hour\n",
    "# Outcome: Comparison table ready.\n",
    "\n",
    "# %%\n",
    "# Task 28: Write Conclusion and Report\n",
    "# Action: Write the conclusions based on the comparison.\n",
    "# Time Estimate: 3 hours\n",
    "# Outcome: Completed report.\n",
    "\n",
    "# %%\n",
    "# Task 29: Finalize Colab Notebook\n",
    "# Action: Ensure the Colab notebook is well-documented and shareable.\n",
    "# Time Estimate: 1 hour\n",
    "# Outcome: Ready-to-submit Colab notebook.\n",
    "\n",
    "# %%\n",
    "# Task 30: Submit Assignment\n",
    "# Action: Share the Colab notebook with the specified email.\n",
    "# Time Estimate: 30 minutes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
